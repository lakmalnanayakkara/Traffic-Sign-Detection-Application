{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8abd146-d4a7-4a55-8ee4-3bd4e571192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "from PyQt5.QtWidgets import (QApplication, QLabel, QVBoxLayout, QWidget, QPushButton, QFileDialog, QHBoxLayout,)\n",
    "from PyQt5.QtGui import (QPixmap, QImage, QColor, QPalette,)\n",
    "from PyQt5.QtCore import Qt\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5ecd8b7-d1a7-4ee3-868b-688410028d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(img, scale_percent) :\n",
    "    # Calculate new size\n",
    "    width = int(img.shape[1] * scale_percent / 100)\n",
    "    height = int(img.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "    # Resize image\n",
    "    resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    return resized\n",
    "\n",
    "def draw_box(img, result, class_list):\n",
    "    # Get information from result\n",
    "    xyxy = result.boxes.xyxy.cpu().numpy()  # Move to CPU and convert to NumPy\n",
    "    confidence = result.boxes.conf.cpu().numpy()  # Move to CPU and convert to NumPy\n",
    "    class_id = result.boxes.cls.cpu().numpy().astype(int)  # Move to CPU and convert to NumPy\n",
    "\n",
    "    # Get Class name\n",
    "    class_name = [class_list[x] for x in class_id]\n",
    "    \n",
    "    # Pack together for easy use\n",
    "    sum_output = list(zip(class_name, confidence, xyxy))\n",
    "    \n",
    "    # Copy image, in case that we need the original image for something\n",
    "    out_image = img.copy()\n",
    "    \n",
    "    for run_output in sum_output:\n",
    "        # Unpack\n",
    "        label, con, box = run_output\n",
    "        # Choose color\n",
    "        box_color = (0, 0, 255)\n",
    "        text_color = (255, 255, 255)\n",
    "        # Draw object box\n",
    "        first_half_box = (int(box[0]), int(box[1]))\n",
    "        second_half_box = (int(box[2]), int(box[3]))\n",
    "        cv2.rectangle(out_image, first_half_box, second_half_box, box_color, 2)\n",
    "        # Create text\n",
    "        text_print = '{label} {con:.2f}'.format(label=label, con=con)\n",
    "        # Locate text position\n",
    "        text_location = (int(box[0]), int(box[1] - 10))\n",
    "        # Get size and baseline\n",
    "        labelSize, baseLine = cv2.getTextSize(text_print, cv2.FONT_HERSHEY_SIMPLEX, 1, 2)\n",
    "        # Draw text's background\n",
    "        cv2.rectangle(out_image,\n",
    "                      (int(box[0]), int(box[1] - labelSize[1] - 10)),\n",
    "                      (int(box[0]) + labelSize[0], int(box[1] + baseLine - 10)),\n",
    "                      box_color, cv2.FILLED)\n",
    "        # Put text\n",
    "        cv2.putText(out_image, text_print, text_location,\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                    text_color, 2, cv2.LINE_AA)\n",
    "    \n",
    "    return out_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88909fd4-7588-4eda-8118-a5b6827ca7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficSignRecognition(QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle(\"Traffic-Signs-Recognition\")\n",
    "        self.setFixedSize(1400, 900)  # Set a fixed size for the window\n",
    "        self.layout = QVBoxLayout()\n",
    "        self.default_image_path = \"interface.jpg\"\n",
    "\n",
    "        # Initialize the text-to-speech engine\n",
    "        self.tts_engine = pyttsx3.init()\n",
    "\n",
    "        self.last_spoken_sign = None\n",
    "        self.last_spoken_time = time.time()\n",
    "\n",
    "        # Title Label\n",
    "        title_label = QLabel(\"Srilankan Traffic Signs Detection System\")\n",
    "        title_label.setAlignment(Qt.AlignHCenter)\n",
    "        title_label.setStyleSheet(\"font-size: 24px; font-weight: bold; margin-top: 10px; margin-bottom: 10px;\")\n",
    "        self.layout.addWidget(title_label)\n",
    "\n",
    "        # Detected Class Label\n",
    "        self.class_label = QLabel()\n",
    "        self.class_label.setAlignment(Qt.AlignCenter)\n",
    "        self.class_label.setStyleSheet(\"border: 1px solid rgb(127, 129, 130); font-size: 16px; font-weight: bold; margin: 10px; padding: 10px; border-radius: 10px;\")\n",
    "        # self.class_label.setFixedHeight(80)\n",
    "        self.layout.addWidget(self.class_label)\n",
    "\n",
    "        # Result Display\n",
    "        self.result_label = QLabel()\n",
    "        self.result_label.setStyleSheet(\"border: 1px solid rgb(127, 129, 130); margin: 10px; padding: 10px; border-radius: 10px; color: rgb(25, 25, 25);\")\n",
    "        self.result_label.setFixedHeight(620)\n",
    "        self.result_label.setAlignment(Qt.AlignCenter)\n",
    "        self.layout.addWidget(self.result_label)\n",
    "\n",
    "        # Set the default image in the result label\n",
    "        default_image = QPixmap(self.default_image_path)\n",
    "        self.result_label.setPixmap(default_image.scaled(self.result_label.width(), self.result_label.height(), Qt.KeepAspectRatio))\n",
    "\n",
    "        # Create a QHBoxLayout for button layout\n",
    "        selection_button_layout = QHBoxLayout()\n",
    "\n",
    "        # Select Image Button\n",
    "        image_button = QPushButton(\"Select Image\")\n",
    "        image_button.setStyleSheet(\"font-size: 18px; font-weight: bold; color:rgb(247, 247, 247) ;padding: 5px; margin-top: 10px; margin-left: 5px; margin-right: 5px; border-radius: 10px; background-color: rgb(127, 129, 130);\")\n",
    "        image_button.clicked.connect(self.select_image)\n",
    "        selection_button_layout.addWidget(image_button)\n",
    "        # self.layout.addWidget(image_button)\n",
    "\n",
    "        # Select Video Button\n",
    "        video_button = QPushButton(\"Select Video\")\n",
    "        video_button.setStyleSheet(\"font-size: 18px; font-weight: bold; color:rgb(247, 247, 247) ; padding: 5px; margin-top: 10px; margin-left: 5px; margin-right: 5px; border-radius: 10px; background-color: rgb(127, 129, 130);\")\n",
    "        video_button.clicked.connect(self.select_video)\n",
    "        selection_button_layout.addWidget(video_button)\n",
    "        # self.layout.addWidget(video_button)\n",
    "\n",
    "        # Start Webcam Button\n",
    "        self.start_webcam_button = QPushButton(\"Real Time Recognition\")\n",
    "        self.start_webcam_button.setStyleSheet(\"font-size: 18px; font-weight: bold; color:rgb(247, 247, 247) ; padding: 5px; margin-top: 10px; margin-left: 5px; margin-right: 5px; border-radius: 10px; background-color: rgb(127, 129, 130);\")\n",
    "        self.start_webcam_button.clicked.connect(self.start_webcam)\n",
    "        selection_button_layout.addWidget(self.start_webcam_button)\n",
    "\n",
    "        # Add button layout to the main layout\n",
    "        self.layout.addLayout(selection_button_layout)\n",
    "\n",
    "        end_button_layout = QVBoxLayout() \n",
    "\n",
    "        # Stop Button\n",
    "        self.stop_button = QPushButton(\"Stop\")\n",
    "        self.stop_button.setStyleSheet(\"font-size: 18px; font-weight: bold; color:rgb(247, 247, 247) ; padding: 5px; margin: 5px; border-radius: 10px; background-color: rgb(127, 129, 130);\")\n",
    "        self.stop_button.clicked.connect(self.stop_prediction)\n",
    "        # self.stop_button.setEnabled(False)\n",
    "        end_button_layout.addWidget(self.stop_button)\n",
    "\n",
    "        # Close Application Button\n",
    "        close_button = QPushButton(\"Exit\")\n",
    "        close_button.setStyleSheet(\"font-size: 18px; font-weight: bold; color:rgb(247, 247, 247) ; padding: 5px; margin: 5px; border-radius: 10px; background-color: rgb(127, 129, 130);\")\n",
    "        close_button.clicked.connect(self.close)\n",
    "        end_button_layout.addWidget(close_button)\n",
    "\n",
    "        self.layout.addLayout(end_button_layout)\n",
    "\n",
    "        self.setLayout(self.layout)\n",
    "\n",
    "        self.video_file = None\n",
    "        self.model = None\n",
    "        self.class_list = None\n",
    "        self.scale_show = 100\n",
    "        self.video_capture = None\n",
    "\n",
    "    def select_image(self):\n",
    "        image_file, _ = QFileDialog.getOpenFileName(self, \"Select Image\", \"\", \"Image Files (*.png *.jpg *.jpeg)\")\n",
    "        if image_file:\n",
    "            self.video_file = None\n",
    "            self.start_prediction_for_image(image_file)\n",
    "\n",
    "    def select_video(self):\n",
    "        video_file, _ = QFileDialog.getOpenFileName(self, \"Select Video\", \"\", \"Video Files (*.mp4 *.avi *.mkv)\")\n",
    "        if video_file:\n",
    "            self.video_file = video_file\n",
    "            self.start_prediction_for_video(video_file)\n",
    "\n",
    "    def start_webcam(self):\n",
    "        self.video_file = 0\n",
    "        self.result_label.setPixmap(QPixmap(self.default_image_path).scaled(self.result_label.width(), self.result_label.height(), Qt.KeepAspectRatio))\n",
    "        # self.start_webcam_button.setEnabled(False)\n",
    "        # self.stop_button.setEnabled(True)\n",
    "        self.start_prediction_for_video(\"webcam\")   \n",
    "\n",
    "    def stop_prediction(self):\n",
    "        self.video_file = None\n",
    "        # self.stop_button.setEnabled(False)\n",
    "        # self.start_webcam_button.setEnabled(True)\n",
    "        self.video_capture.release()\n",
    "        self.result_label.clear()\n",
    "        self.class_label.clear()\n",
    "        self.result_label.setPixmap(QPixmap(self.default_image_path).scaled(self.result_label.width(), self.result_label.height(), Qt.KeepAspectRatio))\n",
    "\n",
    "    def start_prediction_for_image(self, file):\n",
    "        self.model = YOLO(\"/Users/USER/runs/detect/train/weights/best.pt\")\n",
    "        self.class_list = {  \n",
    "            0: 'Children-Crossing',  \n",
    "            1: 'Hospital',  \n",
    "            2: 'Level-Crossing-with-Gates',  \n",
    "            3: 'No-Honking',  \n",
    "            4: 'No-Left-Turn',  \n",
    "            5: 'No-Right-Turn',  \n",
    "            6: 'No-U-Turn',  \n",
    "            7: 'School-Ahead',   \n",
    "        }  \n",
    "\n",
    "        self.video_capture = cv2.VideoCapture(file)\n",
    "\n",
    "        while True:\n",
    "            ret, frame = self.video_capture.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Convert RGBA image to RGB (if required)\n",
    "            if frame.shape[2] == 4:\n",
    "                # print(\"inside\")\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_RGBA2RGB)\n",
    "\n",
    "            results = self.model.predict(frame, verbose=False)\n",
    "            labeled_img = draw_box(frame, results[0], self.class_list)\n",
    "            display_img = resize_image(labeled_img, self.scale_show)\n",
    "\n",
    "            # Convert the image to QImage\n",
    "            rgb_image = cv2.cvtColor(display_img, cv2.COLOR_BGR2RGB)\n",
    "            h, w, ch = rgb_image.shape\n",
    "            bytes_per_line = ch * w\n",
    "            q_image = QImage(rgb_image.data, w, h, bytes_per_line, QImage.Format_RGB888)\n",
    "\n",
    "            # Display the image in the QLabel\n",
    "            pixmap = QPixmap(q_image)\n",
    "            self.result_label.setPixmap(pixmap.scaled(self.result_label.width(), self.result_label.height(), Qt.KeepAspectRatio))\n",
    "\n",
    "            # Update the detected class label\n",
    "            if results[0].boxes is not None and len(results[0].boxes) > 0:\n",
    "                labels = []\n",
    "                for i in range(len(results[0].boxes)):\n",
    "                    class_id = results[0].boxes.cls[i].item()  # Convert tensor to integer\n",
    "                    if class_id in self.class_list:\n",
    "                        class_name = self.class_list[class_id]\n",
    "                        confidence = results[0].boxes.conf[i] * 100\n",
    "                        label = f\"{class_name}: {confidence:.2f}%\"\n",
    "                        labels.append(label)\n",
    "                label_text = \"\\n\".join(labels)\n",
    "            else:\n",
    "                label_text = \"No detection\"\n",
    "            self.class_label.setText(label_text)\n",
    "\n",
    "             # Determine whether to say \"No detection\" or not based on the file type\n",
    "            image_selection = file.endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "\n",
    "            # Process GUI events\n",
    "            QApplication.processEvents()\n",
    "\n",
    "            # Output detected signs using speech\n",
    "            self.speak_detected_signs(results[0].boxes, image_selection)\n",
    "\n",
    "    def start_prediction_for_video(self, file):\n",
    "        self.model = YOLO(\"/Users/USER/runs/detect/train/weights/best.pt\")\n",
    "        self.class_list = {  \n",
    "            0: 'Children-Crossing',  \n",
    "            1: 'Hospital',  \n",
    "            2: 'Level-Crossing-with-Gates',  \n",
    "            3: 'No-Honking',  \n",
    "            4: 'No-Left-Turn',  \n",
    "            5: 'No-Right-Turn',  \n",
    "            6: 'No-U-Turn',  \n",
    "            7: 'School-Ahead',   \n",
    "        }  \n",
    "\n",
    "        if file == \"webcam\":\n",
    "            self.video_capture = cv2.VideoCapture(0)\n",
    "        else:\n",
    "            self.video_capture = cv2.VideoCapture(file)\n",
    "\n",
    "        frames_to_skip = 10  # Process every 10th frame (change this as needed)\n",
    "        reduced_scale_percent = 100  # Adjust this value for desired video quality\n",
    "        frame_count = 0\n",
    "\n",
    "        while True:\n",
    "            ret, frame = self.video_capture.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_count += 1\n",
    "            if self.video_capture:\n",
    "                if frame_count % frames_to_skip != 0:\n",
    "                    continue\n",
    "\n",
    "            # Resize frame to reduce the resolution\n",
    "            reduced_frame = resize_image(frame, reduced_scale_percent)\n",
    "\n",
    "            # Convert RGBA image to RGB (if required)\n",
    "            if reduced_frame.shape[2] == 4:\n",
    "                # print(\"inside\")\n",
    "                reduced_frame = cv2.cvtColor(reduced_frame, cv2.COLOR_RGBA2RGB)\n",
    "\n",
    "            results = self.model.predict(reduced_frame, verbose=False)\n",
    "            labeled_img = draw_box(reduced_frame, results[0], self.class_list)\n",
    "            display_img = resize_image(labeled_img, self.scale_show)\n",
    "\n",
    "            # Convert the image to QImage\n",
    "            rgb_image = cv2.cvtColor(display_img, cv2.COLOR_BGR2RGB)\n",
    "            h, w, ch = rgb_image.shape\n",
    "            bytes_per_line = ch * w\n",
    "            q_image = QImage(rgb_image.data, w, h, bytes_per_line, QImage.Format_RGB888)\n",
    "\n",
    "            # Display the image in the QLabel\n",
    "            pixmap = QPixmap(q_image)\n",
    "            self.result_label.setPixmap(pixmap.scaled(self.result_label.width(), self.result_label.height(), Qt.KeepAspectRatio))\n",
    "\n",
    "            # Update the detected class label\n",
    "            if results[0].boxes is not None and len(results[0].boxes) > 0:\n",
    "                labels = []\n",
    "                for i in range(len(results[0].boxes)):\n",
    "                    class_id = results[0].boxes.cls[i].item()  # Convert tensor to integer\n",
    "                    if class_id in self.class_list:\n",
    "                        class_name = self.class_list[class_id]\n",
    "                        confidence = results[0].boxes.conf[i] * 100\n",
    "                        label = f\"{class_name}: {confidence:.2f}%\"\n",
    "                        labels.append(label)\n",
    "                        results[0].boxes\n",
    "                label_text = \"\\n\".join(labels) if labels else \"No high-confidence detection\"\n",
    "            else:\n",
    "                label_text = \"No detection\"\n",
    "\n",
    "            self.class_label.setText(label_text)\n",
    "\n",
    "             # Determine whether to say \"No detection\" or not based on the file type\n",
    "            image_selection = file.endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "\n",
    "            # Process GUI events\n",
    "            QApplication.processEvents()\n",
    "\n",
    "            # Output detected signs using speech\n",
    "            self.speak_detected_signs(results[0].boxes, image_selection)\n",
    "\n",
    "    def speak_detected_signs(self, boxes, image_selection):\n",
    "        if boxes is not None and len(boxes) > 0:\n",
    "            detected_signs = []\n",
    "            for i in range(len(boxes)):\n",
    "                class_id = boxes.cls[i].item()  # Convert tensor to integer\n",
    "                if class_id in self.class_list:\n",
    "                    class_name = self.class_list[class_id]\n",
    "                    confidence = boxes.conf[i] * 100\n",
    "                    detected_signs.append(class_name)\n",
    "                    \n",
    "            if detected_signs:\n",
    "                current_time = time.time()\n",
    "                if detected_signs != self.last_spoken_sign or current_time - self.last_spoken_time > 5:\n",
    "                    speech_text = \" and \".join(detected_signs)  # Combine multiple detections with \"and\"\n",
    "                    self.tts_engine.say(speech_text)\n",
    "                    self.tts_engine.runAndWait()\n",
    "                    self.last_spoken_sign = detected_signs\n",
    "                    self.last_spoken_time = current_time\n",
    "        else:\n",
    "            if image_selection:\n",
    "                self.tts_engine.say(\"No detection\")\n",
    "                self.tts_engine.runAndWait()\n",
    "\n",
    "    def closeEvent(self, event):\n",
    "        self.video_capture.release()\n",
    "        event.accept()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90cd1971-725b-43fb-a5c4-e77c2623c305",
   "metadata": {},
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\TFProj\\tfvenv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app = QApplication(sys.argv)\n",
    "\n",
    "    # Set custom styles\n",
    "    palette = QPalette()\n",
    "    palette.setColor(QPalette.Window, QColor(245, 245, 245))  # Set background color\n",
    "    palette.setColor(QPalette.WindowText, QColor(51, 51, 51)) \n",
    "    app.setPalette(palette)\n",
    "\n",
    "    gui = TrafficSignRecognition()\n",
    "    gui.show()\n",
    "\n",
    "    sys.exit(app.exec_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973e6b23-514d-414c-a400-90a49a8fd235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfkernel",
   "language": "python",
   "name": "tfkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
